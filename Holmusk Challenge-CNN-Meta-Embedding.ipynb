{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6cd1d639",
   "metadata": {},
   "source": [
    "Meta-Embedding\n",
    "\n",
    "This experiment is to assess the feasibility of meta-embeddings in word representation of clinical notes. \n",
    "Reference Paper: https://arxiv.org/pdf/1804.05262.pdf\n",
    "\n",
    "- averaging and concatenation of different word2vec models\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "4a699ed5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gensim\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from keras import regularizers\n",
    "from keras.layers import Conv1D, MaxPooling1D, Embedding, Conv2D, Reshape, MaxPool2D, Concatenate\n",
    "from keras.layers import Dense, Input, Flatten, Dropout, merge\n",
    "from keras.models import Model\n",
    "from keras.optimizers import Adam\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.utils import to_categorical\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "from sklearn.metrics import classification_report, confusion_matrix, accuracy_score, roc_auc_score\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "import matplotlib.pyplot as plt\n",
    "import gc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "abb3b46c",
   "metadata": {},
   "outputs": [],
   "source": [
    "medical_notes = pd.read_csv(r\"C:\\Users\\61102\\PSU-PhD\\Holmusk\\ClinNotes_Preprocess.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "075c74eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "medical_notes['category'].replace({'Cardiovascular / Pulmonary': 1, 'Neurology': 2, 'Gastroenterology': 3},inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "bcdea379",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "13617\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\61102\\anaconda3\\envs\\GPU-env\\lib\\site-packages\\ipykernel_launcher.py:8: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  \n"
     ]
    }
   ],
   "source": [
    "## split to train, test, val\n",
    "from sklearn.model_selection import train_test_split\n",
    "train_df, test_df = train_test_split(medical_notes, test_size=0.2, random_state=2018)\n",
    "train_df, val_df = train_test_split(train_df, test_size=0.1, random_state=2018)\n",
    "\n",
    "tokenizer = RegexpTokenizer(r'\\w+')\n",
    "train_df[\"tokens\"] = train_df[\"preprocess_notes\"].apply(tokenizer.tokenize)\n",
    "test_df[\"tokens\"] = test_df[\"preprocess_notes\"].apply(tokenizer.tokenize)\n",
    "val_df[\"tokens\"] = val_df[\"preprocess_notes\"].apply(tokenizer.tokenize)\n",
    "\n",
    "all_words = [word for tokens in train_df[\"tokens\"] for word in tokens]\n",
    "all_words += [word for tokens in test_df[\"tokens\"] for word in tokens]\n",
    "all_words += [word for tokens in val_df[\"tokens\"] for word in tokens]\n",
    "VOCAB = sorted(list(set(all_words)))\n",
    "print(len(VOCAB))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "d4af896a",
   "metadata": {},
   "outputs": [],
   "source": [
    "EMBEDDING_DIM = 300\n",
    "MAX_SEQUENCE_LENGTH = 500\n",
    "VOCAB_SIZE = len(VOCAB)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "d2531498",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 13626 unique tokens.\n"
     ]
    }
   ],
   "source": [
    "tokenizer = Tokenizer(num_words=VOCAB_SIZE)\n",
    "tokenizer.fit_on_texts(train_df[\"preprocess_notes\"].tolist())\n",
    "tokenizer.fit_on_texts(test_df[\"preprocess_notes\"].tolist())\n",
    "tokenizer.fit_on_texts(val_df[\"preprocess_notes\"].tolist())\n",
    "\n",
    "train_sequences = tokenizer.texts_to_sequences(train_df[\"preprocess_notes\"].tolist())\n",
    "test_sequences = tokenizer.texts_to_sequences(test_df[\"preprocess_notes\"].tolist())\n",
    "val_sequences = tokenizer.texts_to_sequences(val_df[\"preprocess_notes\"].tolist())\n",
    "\n",
    "word_index = tokenizer.word_index\n",
    "print('Found %s unique tokens.' % len(word_index))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "2712a9c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_X = pad_sequences(train_sequences, maxlen=MAX_SEQUENCE_LENGTH)\n",
    "test_X = pad_sequences(test_sequences, maxlen=MAX_SEQUENCE_LENGTH)\n",
    "val_X = pad_sequences(val_sequences, maxlen=MAX_SEQUENCE_LENGTH)\n",
    "\n",
    "labelencoder_Y = LabelEncoder()\n",
    "labels_train = labelencoder_Y.fit_transform(train_df[\"category\"])\n",
    "train_y = to_categorical(labels_train, num_classes=3)\n",
    "labels_test = labelencoder_Y.fit_transform(test_df[\"category\"])\n",
    "test_y = to_categorical(labels_test, num_classes=3)\n",
    "labels_val = labelencoder_Y.fit_transform(val_df[\"category\"])\n",
    "val_y = to_categorical(labels_val, num_classes=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "1ceb7361",
   "metadata": {},
   "outputs": [],
   "source": [
    "word2vec_path = r\"C:\\Users\\61102\\PSU-PhD\\Holmusk\\GoogleNews-vectors-negative300.bin\"\n",
    "word2vec = gensim.models.KeyedVectors.load_word2vec_format(word2vec_path, binary=True, no_header=False,unicode_errors='ignore')\n",
    "embedding_matrix_1 = np.zeros((len(word_index) + 1, EMBEDDING_DIM))\n",
    "for word, index in word_index.items():\n",
    "    embedding_matrix_1[index, :] = embedding_matrix_1[index, :] = word2vec[word] if word in word2vec else np.random.rand(\n",
    "        EMBEDDING_DIM)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "d3d477e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "word2vec_path = r\"C:\\Users\\61102\\PSU-PhD\\Holmusk\\ClinNotes_w2v.bin\"\n",
    "word2vec = gensim.models.KeyedVectors.load_word2vec_format(word2vec_path, binary=True, no_header=False,unicode_errors='ignore')\n",
    "embedding_matrix_2 = np.zeros((len(word_index) + 1, EMBEDDING_DIM))\n",
    "for word, index in word_index.items():\n",
    "    embedding_matrix_2[index, :] = embedding_matrix_2[index, :] = word2vec[word] if word in word2vec else np.random.rand(\n",
    "        EMBEDDING_DIM)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "bd37fd6d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(13627, 300)"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embedding_matrix = np.mean([embedding_matrix_1, embedding_matrix_2], axis = 0)\n",
    "del embedding_matrix_1, embedding_matrix_2\n",
    "gc.collect()\n",
    "np.shape(embedding_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "13131da7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def ConvNet(embeddings, max_sequence_length, num_words, embedding_dim, labels_index, trainable=False, extra_conv=True):\n",
    "    embedding_layer = Embedding(num_words,\n",
    "                                embedding_dim,\n",
    "                                weights=[embeddings],\n",
    "                                input_length=max_sequence_length,\n",
    "                                trainable=trainable)\n",
    "\n",
    "    sequence_input = Input(shape=(max_sequence_length,), dtype='int32')\n",
    "    embedded_sequences = embedding_layer(sequence_input)\n",
    "\n",
    "    # Yoon Kim model (https://arxiv.org/abs/1408.5882)\n",
    "    convs = []\n",
    "    filter_sizes = [3, 4, 5]\n",
    "\n",
    "    for filter_size in filter_sizes:\n",
    "        l_conv = Conv1D(filters=128, kernel_size=filter_size, activation='relu')(embedded_sequences)\n",
    "        l_pool = MaxPooling1D(pool_size=3)(l_conv)\n",
    "        convs.append(l_pool)\n",
    "\n",
    "    l_merge = merge.concatenate(inputs=convs, axis=1)\n",
    "\n",
    "    # add a 1D convnet with global maxpooling, instead of Yoon Kim model\n",
    "    conv = Conv1D(filters=128, kernel_size=3, activation='relu')(embedded_sequences)\n",
    "    pool = MaxPooling1D(pool_size=3)(conv)\n",
    "\n",
    "    if extra_conv:\n",
    "        x = Dropout(0.5)(l_merge)\n",
    "    else:\n",
    "        # Original Yoon Kim model\n",
    "        x = Dropout(0.5)(pool)\n",
    "    x = Flatten()(x)\n",
    "    x = Dense(128, activation='relu')(x)\n",
    "    # x = Dropout(0.5)(x)\n",
    "\n",
    "    preds = Dense(labels_index, activation='softmax')(x)\n",
    "\n",
    "    model = Model(sequence_input, preds)\n",
    "    model.compile(loss='categorical_crossentropy',\n",
    "                  optimizer='adam',\n",
    "                  metrics=['acc'])\n",
    "    model.summary()\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "68e619dc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_2\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_2 (InputLayer)         (None, 500)               0         \n",
      "_________________________________________________________________\n",
      "embedding_2 (Embedding)      (None, 500, 300)          4088100   \n",
      "_________________________________________________________________\n",
      "conv1d_8 (Conv1D)            (None, 498, 128)          115328    \n",
      "_________________________________________________________________\n",
      "max_pooling1d_8 (MaxPooling1 (None, 166, 128)          0         \n",
      "_________________________________________________________________\n",
      "dropout_2 (Dropout)          (None, 166, 128)          0         \n",
      "_________________________________________________________________\n",
      "flatten_2 (Flatten)          (None, 21248)             0         \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 128)               2719872   \n",
      "_________________________________________________________________\n",
      "dense_4 (Dense)              (None, 3)                 387       \n",
      "=================================================================\n",
      "Total params: 6,923,687\n",
      "Trainable params: 6,923,687\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model = ConvNet(embedding_matrix, MAX_SEQUENCE_LENGTH, len(word_index) + 1, EMBEDDING_DIM,\n",
    "                len(list(train_df[\"category\"].unique())), True, False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "1b8704e7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\61102\\anaconda3\\envs\\GPU-env\\lib\\site-packages\\tensorflow_core\\python\\framework\\indexed_slices.py:433: UserWarning: Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.\n",
      "  \"Converting sparse IndexedSlices to a dense Tensor of unknown shape. \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 588 samples, validate on 66 samples\n",
      "Epoch 1/20\n",
      "588/588 [==============================] - 0s 363us/step - loss: 2.1791 - acc: 0.3095 - val_loss: 3.2794 - val_acc: 0.2576\n",
      "Epoch 2/20\n",
      "588/588 [==============================] - 0s 149us/step - loss: 2.9724 - acc: 0.3112 - val_loss: 1.3875 - val_acc: 0.4394\n",
      "Epoch 3/20\n",
      "588/588 [==============================] - 0s 132us/step - loss: 1.1592 - acc: 0.4320 - val_loss: 1.0987 - val_acc: 0.2879\n",
      "Epoch 4/20\n",
      "588/588 [==============================] - 0s 127us/step - loss: 1.0935 - acc: 0.3929 - val_loss: 1.0988 - val_acc: 0.2879\n",
      "Epoch 5/20\n",
      "588/588 [==============================] - 0s 127us/step - loss: 1.0901 - acc: 0.4167 - val_loss: 1.0901 - val_acc: 0.4091\n",
      "Epoch 6/20\n",
      "588/588 [==============================] - 0s 128us/step - loss: 1.0934 - acc: 0.3912 - val_loss: 1.0987 - val_acc: 0.2879\n",
      "Epoch 7/20\n",
      "588/588 [==============================] - 0s 127us/step - loss: 1.0931 - acc: 0.3878 - val_loss: 1.1008 - val_acc: 0.4545\n",
      "Epoch 8/20\n",
      "588/588 [==============================] - 0s 127us/step - loss: 1.0915 - acc: 0.3861 - val_loss: 1.0986 - val_acc: 0.2879\n",
      "Epoch 9/20\n",
      "588/588 [==============================] - 0s 128us/step - loss: 1.0932 - acc: 0.4201 - val_loss: 1.1217 - val_acc: 0.4545\n",
      "Epoch 10/20\n",
      "588/588 [==============================] - 0s 128us/step - loss: 1.0753 - acc: 0.4371 - val_loss: 1.1473 - val_acc: 0.4394\n",
      "Epoch 11/20\n",
      "588/588 [==============================] - 0s 127us/step - loss: 1.0912 - acc: 0.4405 - val_loss: 1.0982 - val_acc: 0.4394\n",
      "Epoch 12/20\n",
      "588/588 [==============================] - 0s 130us/step - loss: 1.0917 - acc: 0.4422 - val_loss: 1.1539 - val_acc: 0.4394\n",
      "Epoch 13/20\n",
      "588/588 [==============================] - 0s 136us/step - loss: 1.0839 - acc: 0.4405 - val_loss: 1.0820 - val_acc: 0.4394\n",
      "Epoch 14/20\n",
      "588/588 [==============================] - 0s 128us/step - loss: 1.0901 - acc: 0.4405 - val_loss: 1.1364 - val_acc: 0.4394\n",
      "Epoch 15/20\n",
      "588/588 [==============================] - 0s 129us/step - loss: 1.0883 - acc: 0.4405 - val_loss: 1.0978 - val_acc: 0.4394\n",
      "Epoch 16/20\n",
      "588/588 [==============================] - 0s 128us/step - loss: 1.0866 - acc: 0.4405 - val_loss: 1.0973 - val_acc: 0.4394\n",
      "Epoch 17/20\n",
      "588/588 [==============================] - 0s 128us/step - loss: 1.0926 - acc: 0.4405 - val_loss: 1.0976 - val_acc: 0.4394\n",
      "Epoch 18/20\n",
      "588/588 [==============================] - 0s 128us/step - loss: 1.0936 - acc: 0.4405 - val_loss: 1.0967 - val_acc: 0.4394\n",
      "Epoch 19/20\n",
      "588/588 [==============================] - 0s 126us/step - loss: 1.0928 - acc: 0.4405 - val_loss: 1.0960 - val_acc: 0.4394\n",
      "Epoch 20/20\n",
      "588/588 [==============================] - 0s 127us/step - loss: 1.0910 - acc: 0.4405 - val_loss: 1.0970 - val_acc: 0.4394\n"
     ]
    }
   ],
   "source": [
    "history = model.fit(train_X, train_y, batch_size=512, epochs=20, validation_data=(val_X, val_y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "dffb88ff",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classification Report\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.51      1.00      0.67        83\n",
      "           1       0.00      0.00      0.00        42\n",
      "           2       0.00      0.00      0.00        39\n",
      "\n",
      "    accuracy                           0.51       164\n",
      "   macro avg       0.17      0.33      0.22       164\n",
      "weighted avg       0.26      0.51      0.34       164\n",
      "\n",
      "Confusion Matrix\n",
      "[[83  0  0]\n",
      " [42  0  0]\n",
      " [39  0  0]]\n",
      "Accuracy Score\n",
      "0.5060975609756098\n",
      "ROC-AUC Score\n",
      "0.5502289652615547\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\61102\\anaconda3\\envs\\GPU-env\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1248: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "C:\\Users\\61102\\anaconda3\\envs\\GPU-env\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1248: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "C:\\Users\\61102\\anaconda3\\envs\\GPU-env\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1248: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    }
   ],
   "source": [
    "preds = model.predict(test_X)\n",
    "print(\"Classification Report\")\n",
    "print(classification_report(np.argmax(test_y, axis=1), np.argmax(preds, axis=1)))\n",
    "print(\"Confusion Matrix\")\n",
    "print(confusion_matrix(np.argmax(test_y, axis=1), np.argmax(preds, axis=1)))\n",
    "print(\"Accuracy Score\")\n",
    "print(accuracy_score(np.argmax(test_y, axis=1), np.argmax(preds, axis=1), normalize=True))\n",
    "print(\"ROC-AUC Score\")\n",
    "print(roc_auc_score(test_y, preds, multi_class='ovr'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce285097",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
